<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Simone Rossetti </title> <meta name="author" content="Simone Rossetti"> <meta name="description" content="publications by categories in reversed chronological order."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rossettisimone.github.io/publications/"> <script src="/assets/js/theme.js?v=a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Simone</span> Rossetti </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order.</p> </header> <article> <script src="/assets/js/bibsearch.js?v=1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">unpublished</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cabbage.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cabbage.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rossetti2025cabbage" class="col-sm-8"> <div class="title">CABBAGE: Comprehensive Agricultural Benchmark Backed by AI-Guided Evaluation</div> <div class="author"> <em>Simone Rossetti<sup>*</sup></em>, Paolo Gatti<sup>*</sup>, and Davide Palleschi<sup>*</sup> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* DeepPlants"> </i> </div> <div class="periodical"> <em>More Information</em> can be <a href="https://huggingface.co/datasets/deepplants/cabbage" rel="external nofollow noopener" target="_blank">found here</a>. , 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#FF0000"> <a href="https://iris.uniroma1.it/" rel="external nofollow noopener" target="_blank">IRIS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/reducing.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="reducing.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rossetti2025reducing" class="col-sm-8"> <div class="title">Reducing supervision in semantic segmentation through advancements in bayesian prior modelling</div> <div class="author"> <em>Simone Rossetti</em> </div> <div class="periodical"> <em></em> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://iris.uniroma1.it/handle/11573/1733365" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://iris.uniroma1.it/retrieve/335f5d4e-0c5e-459a-b1ed-e58a706625a4/Tesi_dottorato_Rossetti.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=aynWg48AAAAJ&amp;citation_for_view=aynWg48AAAAJ:UeHWp8X0CEIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Over the past few years, semantic segmentation has witnessed significant advancements, particularly with the emergence of Vision Transformers (ViTs). However, from 2021 through 2022, when this research was begun, the adoption of ViTs in semantic segmentation was not yet widespread. The field continued to face challenges due to the labour-intensive and costly nature of annotating data for semantic segmentation. This thesis addresses these challenges by exploring reduced-supervision and unsupervised methodologies to make semantic segmentation more efficient and accessible through three interconnected research projects. In the first study, we investigate the robustness of classification pre-trained deep neural networks for semantic segmentation without spatial guidance on object positions—a common challenge in weakly supervised semantic segmentation (WSSS). We address this by extracting high-level information encoded within model representations through low-level information degradation and multi-view information bottleneck techniques. By leveraging geometric priors on image composition—specifically, the principle of geometric equivariance under affine transformations—we enhance the model’s ability to segment images accurately. Our empirical results demonstrate that ViTs, when combined with appropriate computation of Class Activation Maps (CAMs), are significantly more effective in achieving high-quality WSSS than the previously favoured deep convolutional networks (CNNs). Building on these findings and the limitations of current approaches, our second study aims to mitigate the effects of the lack of spatial information in WSSS with a prior assumption about the spatial distribution of categories across natural images. We propose that objects appear at different scales within images, either in the foreground or background, leading to a similar spatial distribution for each category over a large set of images. By incorporating this prior, we avoid the side effects of unbalanced data distribution among visual concepts and enhance model generalization in WSSS. Our method achieves new state-of-the-art performance on several benchmarks. We model this prior through class frequencies and matrix balancing, an approach derived from optimal transport theory. Unlike contrastive learning methods, our approach operates efficiently with small batches without memory bank requirements, demonstrating the significant potential of cluster-based principles to enhance WSSS, reaching results comparable to fully supervised methods. The third study explores the realm of unsupervised semantic segmentation (USS) by introducing a deep recursive spectral clustering technique that leverages the hypothesis that semantics is hierarchical. While conventional methods often rely on dataset-specific predefined assumptions, such as object-part decomposition and salient semantic regions, our data-driven approach segments images at multiple levels of granularity without prior knowledge of the scene’s structure. This algebraic method recursively refines segmentation based on the inherent semantic properties of the data, providing a flexible and robust way of grouping pixels. We experimentally demonstrate that the method excels in discovering fine and coarse semantic structures in a fully unsupervised manner, offering substantial improvements over traditional models that often struggle with granularity and require dataset-specific priors hindering scalability. Altogether, these studies advance semantic segmentation by progressively reducing the level of required supervision and demonstrating the effectiveness of minimizing reliance on pixel-level annotations through appropriate prior assumption modelling. These contributions align with the evolving trends in semantic segmentation during this research and pave the way for future developments in computer vision.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rossetti2025reducing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Reducing supervision in semantic segmentation through advancements in bayesian prior modelling}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rossetti, Simone}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{PhD Thesis}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Universit{\`a} degli Studi di Roma" La Sapienza"}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#8F00FF"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/hierarchy.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="hierarchy.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rossetti2024hierarchy" class="col-sm-8"> <div class="title">Hierarchy-Agnostic Unsupervised Segmentation: Parsing Semantic Image Structure</div> <div class="author"> <em>Simone Rossetti<sup>†*</sup></em> and <a href="https://scholar.google.com/citations?user=VkYspW0AAAAJ&amp;hl" rel="external nofollow noopener" target="_blank">Fiora Pirri<sup>†*</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† DIAG, Sapienza University of Rome&lt;br&gt;* DeepPlants"> </i> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>. <em>More Information</em> can be <a href="https://github.com/deepplants/recursive-deep-spectral-clustering" rel="external nofollow noopener" target="_blank">found here</a>. , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.52202/079017-3139" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.neurips.cc/paper/2024/hash/b31c332c4cebcec31b788400b47c94b3-Abstract.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://proceedings.neurips.cc/paper/2024/file/b31c332c4cebcec31b788400b47c94b3-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/b31c332c4cebcec31b788400b47c94b3-Supplemental-Conference.zip" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.52202/079017-3139" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=aynWg48AAAAJ&amp;citation_for_view=aynWg48AAAAJ:qjMakFHDy7sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Unsupervised semantic segmentation aims to discover groupings within images, capturing objects’ view-invariance without external supervision. Moreover, this task is inherently ambiguous due to the varying levels of semantic granularity. Existing methods often bypass this ambiguity using dataset-specific priors. In our research, we address this ambiguity head-on and provide a universal tool for pixel-level semantic parsing of images guided by the latent representations encoded in self-supervised models. We introduce a novel algebraic approach that recursively decomposes an image into nested subgraphs, dynamically estimating their count and ensuring clear separation.The innovative approach identifies scene-specific primitives and constructs a hierarchy-agnostic tree of semantic regions from the image pixels. The model captures fine and coarse semantic details, producing a nuanced and unbiased segmentation. We present a new metric for estimating the quality of the semantic segmentation of discovered elements on different levels of the hierarchy. The metric validates the intrinsic nature of the compositional relations among parts, objects, and scenes in a hierarchy-agnostic domain. Our results prove the power of this methodology, uncovering semantic regions without prior definitions and scaling effectively across various datasets. This robust framework for unsupervised image segmentation proves more accurate semantic hierarchical relationships between scene elements than traditional algorithms. The experiments underscore its potential for broad applicability in image analysis tasks, showcasing its ability to deliver a detailed and unbiased segmentation that surpasses existing unsupervised methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rossetti2024hierarchy</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rossetti, Simone and Pirri, Fiora}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hierarchy-Agnostic Unsupervised Segmentation: Parsing Semantic Image Structure}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.52202/079017-3139}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#FFA500"> <a href="https://iccv.thecvf.com/" rel="external nofollow noopener" target="_blank">ICCV</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/agm.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="agm.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="sama2023a" class="col-sm-8"> <div class="title">A new Large Dataset and a Transfer Learning Methodology for Plant Phenotyping in Vertical Farms</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=dma4OokAAAAJ" rel="external nofollow noopener" target="_blank">Nico Samà<sup>*</sup></a>, Etienne David°, <em>Simone Rossetti<sup>†*</sup></em>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Alessandro Antona°, Benjamin Franchetti°, Fiora Pirri&lt;sup&gt;†*&lt;/sup&gt;' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† DIAG, Sapienza University of Rome&lt;br&gt;* DeepPlantse&lt;br&gt;° AgricolaModerna"> </i> </div> <div class="periodical"> <em>In IEEE/CVF International Conference on Computer Vision Workshops</em>. <em>More Information</em> can be <a href="https://github.com/deepplants/agm-plant-classification" rel="external nofollow noopener" target="_blank">found here</a>. , Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ICCVW60793.2023.00061" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Sama_A_new_Large_Dataset_and_a_Transfer_Learning_Methodology_for_ICCVW_2023_paper.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/papers/Sama_A_new_Large_Dataset_and_a_Transfer_Learning_Methodology_for_ICCVW_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICCVW60793.2023.00061" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=aynWg48AAAAJ&amp;citation_for_view=aynWg48AAAAJ:2osOgNQ5qMEC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-12-4285F4?logo=googlescholar&amp;labelColor=beige" alt="12 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Vertical farming has emerged as a solution to enhance crop cultivation efficiency and overcome limitations in conventional farming methods. Yet, abiotic stresses significantly impact crop quality and increase the risk of food loss. The integration of advanced automation, sensor technology, and deep learning models offers a promising solution for quality monitoring addressing the limitations of stress-specific approaches. Due to the large range of possible quality issues, there is a need for a general method. This study proposes a new plant canopy dataset, dubbed AGM of 1M images, annotated with 18 classes, an in-depth analysis of its quality for its use in transfer learning, and a methodology for detecting canopy stresses in vertical farming. The present study trains ViTbase8, ViTsmall8, and ResNet50 both on ImageNet and the proposed dataset on crop classification. Features from AGM and ImageNet are used for a downstream task on healthy and stress detection using a small annotated validation dataset obtaining 0.97%, 0.93%, and 0.92% best accuracy with the AGM features. We compare with standard datasets like Cassava, PlantDoc, and RicePlant obtaining significant accuracy. This research contributes to improved crop quality, prolonged shelf life, and optimized nutrient content in vertical farming, enhancing our understanding of abiotic stress management.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sama2023a</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Samà, Nico and David°, Etienne and Rossetti, Simone and Antona°, Alessandro and Franchetti°, Benjamin and Pirri, Fiora}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A new Large Dataset and a Transfer Learning Methodology for Plant Phenotyping in Vertical Farms}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/CVF International Conference on Computer Vision Workshops}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{540-551}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICCVW60793.2023.00061}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#FF0000"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/pc2m.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pc2m.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rossetti2023removing" class="col-sm-8"> <div class="title">Removing supervision in semantic segmentation with local-global matching and area balancing</div> <div class="author"> <em>Simone Rossetti<sup>†*</sup></em>, <a href="https://scholar.google.com/citations?hl=en&amp;user=dma4OokAAAAJ" rel="external nofollow noopener" target="_blank">Nico Samà<sup>*</sup></a>, and <a href="https://scholar.google.com/citations?user=VkYspW0AAAAJ&amp;hl" rel="external nofollow noopener" target="_blank">Fiora Pirri<sup>†*</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† DIAG, Sapienza University of Rome&lt;br&gt;* DeepPlants"> </i> </div> <div class="periodical"> <em>arXiv preprint arXiv:2303.17410</em>. <em>More Information</em> can be <a href="https://github.com/deepplants/PC2M" rel="external nofollow noopener" target="_blank">found here</a>. , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2303.17410" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2303.17410" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=aynWg48AAAAJ&amp;citation_for_view=aynWg48AAAAJ:9yKSN-GCB0IC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rossetti2023removing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Removing supervision in semantic segmentation with local-global matching and area balancing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rossetti, Simone and Sam{\`a}, Nico and Pirri, Fiora}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2303.17410}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#070bff"> <a href="https://eccv.ecva.net/" rel="external nofollow noopener" target="_blank">ECCV</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/vitpcm.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vitpcm.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rossetti2022max" class="col-sm-8"> <div class="title">Max Pooling with Vision Transformers Reconciles Class and Shape in Weakly Supervised Semantic Segmentation</div> <div class="author"> <em>Simone Rossetti<sup>†*</sup></em>, Damiano Zappia<sup>*</sup>, <a href="https://scholar.google.com/citations?user=Qaq3N3cAAAAJ&amp;hl" rel="external nofollow noopener" target="_blank">Marta Sanzari<sup>*</sup></a>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Marco Schaerf&lt;sup&gt;†*&lt;/sup&gt;, Fiora Pirri&lt;sup&gt;†*&lt;/sup&gt;' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="† DIAG, Sapienza University of Rome&lt;br&gt;* DeepPlants"> </i> </div> <div class="periodical"> <em>In Computer Vision – ECCV</em>. <em>More Information</em> can be <a href="https://github.com/deepplants/ViT-PCM/" rel="external nofollow noopener" target="_blank">found here</a>. , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-031-20056-4_26" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-20056-4_26" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2210.17400" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136900442-supp.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://drive.google.com/file/d/1wcam_nS2Tgs_R_wKyYcbHBxW5Cy_DdEI/view?usp=share_link" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://drive.google.com/file/d/1VrA9T2g8HdYxbVKMP0DbL42Fk8gyRwGu/view?usp=share_link" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1007/978-3-031-20056-4_26" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=aynWg48AAAAJ&amp;citation_for_view=aynWg48AAAAJ:d1gkVwhDpl0C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-99-4285F4?logo=googlescholar&amp;labelColor=beige" alt="99 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Weakly Supervised Semantic Segmentation (WSSS) research has explored many directions to improve the typical pipeline CNN plus class activation maps (CAM) plus refinements, given the image-class label as the only supervision. Though the gap with the fully supervised methods is reduced, further abating the spread seems unlikely within this framework. On the other hand, WSSS methods based on Vision Transformers (ViT) have not yet explored valid alternatives to CAM. ViT features have been shown to retain a scene layout, and object boundaries in self-supervised learning. To confirm these findings, we prove that the advantages of transformers in self-supervised methods are further strengthened by Global Max Pooling (GMP), which can leverage patch features to negotiate pixel-label probability with class probability. This work proposes a new WSSS method dubbed ViT-PCM (ViT Patch-Class Mapping), not based on CAM. The end-to-end presented network learns with a single optimization process, refined shape and proper localization for segmentation masks. Our model outperforms the state-of-the-art on baseline pseudo-masks (BPM), where we achieve 69.3% mIoU on PascalVOC 2012 val set. We show that our approach has the least set of parameters, though obtaining higher accuracy than all other approaches. In a sentence, quantitative and qualitative results of our method reveal that ViT-PCM is an excellent alternative to CNN-CAM based architectures.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rossetti2022max</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rossetti, Simone and Zappia, Damiano and Sanzari, Marta and Schaerf, Marco and Pirri, Fiora}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Max Pooling with Vision Transformers Reconciles Class and Shape in Weakly Supervised Semantic Segmentation}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Computer Vision -- ECCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Nature Switzerland}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Cham}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{446--463}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-031-20056-4}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-031-20056-4_26}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Youtube-VOS</abbr> <figure> <picture> <img src="/assets/img/publication_preview/youtube-vis.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="youtube-vis.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rossettivideo" class="col-sm-8"> <div class="title">Video Instance segmentation Challenge 2021 with Y oloV 4</div> <div class="author"> <em>Simone Rossetti<sup>*</sup></em>, Temirlan Zharkynbek<sup>*</sup>, and <a href="https://scholar.google.com/citations?user=VkYspW0AAAAJ&amp;hl" rel="external nofollow noopener" target="_blank">Fiora Pirri<sup>*</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="* DIAG, Sapienza University of Rome"> </i> </div> <div class="periodical"> <em>Youtube-VOS Challenge</em>. <em>More Information</em> can be <a href="https://youtube-vos.org/challenge/2021/" rel="external nofollow noopener" target="_blank">found here</a>. , 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://youtube-vos.org/assets/challenge/2021/reports/VIS_10_Rossetti.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=aynWg48AAAAJ&amp;citation_for_view=aynWg48AAAAJ:u5HHmVD_uO8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rossettivideo</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Video Instance segmentation Challenge 2021 with Y oloV 4}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rossetti, Simone and Zharkynbek, Temirlan and Pirri, Fiora}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Youtube-VOS Challenge}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Simone Rossetti. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=6f508d74becd347268a7f822bca7309d"></script> <script defer src="/assets/js/newsletter.js?v=c3d0931971ee96e9df74ba70526c3130"></script> </body> </html>