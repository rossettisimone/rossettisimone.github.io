<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Simone Rossetti </title> <meta name="author" content="Simone Rossetti"> <meta name="description" content="Personal website of Simone Rossetti, AI Research Engineer working on computer vision and vision-language learning. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rossettisimone.github.io/"> <script src="/assets/js/theme.js?v=a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Simone</span> Rossetti </h1> <p class="desc">Applied Researcher &amp; Co-Founder @ <a href="https://www.deepplants.com/" rel="external nofollow noopener" target="_blank">DeepPlants</a> ¬∑ AI @ <a href="https://www.diag.uniroma1.it/" rel="external nofollow noopener" target="_blank">DIAG</a> &amp; <a href="https://alcorlab.diag.uniroma1.it/" rel="external nofollow noopener" target="_blank">ALCOR Lab</a></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.jpg?v=f21d0ec8208e9ae848df5dcbef53a53e" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>‚¨ÜÔ∏è Rainbow Mountain, Peru</p> <p>üìç Rome, Italy</p> </div> </div> <div class="clearfix"> <p>üëã Hi, I‚Äôm Simone, an AI Research Engineer working at the intersection of <strong>Computer Vision</strong> and <strong>Natural Language Processing</strong>. My work focuses on <strong>representation learning</strong>, <strong>vision-language alignment</strong>, and <strong>weakly, semi-, and self-supervised learning</strong> for semantic and instance-level visual understanding.</p> <p>üìö My research interests lie in <strong>Multimodal Learning</strong>, <strong>Vision-Language Models (VLMs)</strong>, and <strong>Vision-Language-Action Models (VLAMs)</strong>. I am particularly interested in grounding language into <strong>dense visual predictions</strong> (segmentation, tracking, affordances) and leveraging <strong>foundation models</strong> for zero- and few-shot transfer in structured vision tasks. A recurring theme is <strong>uncertainty modeling and probabilistic priors</strong> to improve robustness, calibration, and data efficiency under limited or noisy supervision.</p> <p>üöÄ I am a <strong>co-founder of</strong> <a href="https://www.deepplants.com/" rel="external nofollow noopener" target="_blank">DeepPlants</a>, where I led research and engineering teams building <strong>production-grade, agentic AI systems</strong> for micro-farming management, plant phenotyping, and agri-tech automation. My experience spans the full <strong>research-to-production pipeline</strong>, from dataset design and large-scale multi-GPU training to model optimization and real-world deployment.</p> <p>üîô Previously, I was an <strong>AI Research Fellow at</strong> <a href="https://alcorlab.diag.uniroma1.it/" rel="external nofollow noopener" target="_blank">ALCOR Lab</a> (Sapienza University of Rome), contributing to peer-reviewed research in computer vision, with a focus on <strong>instance segmentation and tracking</strong> and <strong>activity recognition</strong>.</p> <p>üéì I earned a <strong>PhD in Computer Science Engineering</strong> from <a href="https://www.diag.uniroma1.it/" rel="external nofollow noopener" target="_blank">DIAG</a>, Sapienza University of Rome. My doctoral research focused on <strong>reducing supervision in semantic segmentation</strong> through Bayesian prior modeling and structured regularization. I hold an MSc in <strong>AI &amp; Robotics</strong> and a BSc in <strong>Computer Engineering</strong>, with a background in automation and perception-action systems.</p> <p>üìÑ My work has been presented at <strong>NeurIPS, ECCV, and ICCV</strong>. Selected publications and highlights are available on the <a href="/publications/">Publications page</a>.</p> <p>üìÆ For collaborations reach out at <code class="language-plaintext highlighter-rouge">simone[dot]rossetti[at]live[dot]com</code>.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Mar 01, 2025</th> <td> <a class="news-title" href="/news/announcement_2/">CABBO applying to COSMIC and SmarTerra open calls</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 29, 2025</th> <td> <a class="news-title" href="/news/announcement_4/">Lessons learned while designing a multimodal benchmark for agricultural decision support</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 20, 2025</th> <td> <a class="news-title" href="/news/announcement_3/">CABBO ‚Äì multimodal AI agent for EU micro-farming</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 15, 2024</th> <td> Since <strong>September 2024</strong> I have been leading the <strong>multimodal learning</strong> team at <strong>DeepPlants</strong>, focusing on combining <strong>vision, language and agronomic signals</strong> to build robust, data-efficient models for agricultural applications. </td> </tr> </table> </div> </div> <h2> <a href="/blog/" style="color: inherit">latest posts</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jan 29, 2026</th> <td> <a class="news-title" href="/blog/2026/the-state-of-digitalisation-in-eu-agriculture/">The State of Digitalisation in EU Agriculture: What the Data Finally Shows</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 22, 2026</th> <td> <a class="news-title" href="/blog/2026/the-most-insane-machine/">The Most Insane Machine on Earth: Inside ASML‚Äôs EUV Lithography Systems</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 21, 2026</th> <td> <a class="news-title" href="/blog/2026/how-a-token-first-language/">How a Token-First Language Axis Is Reshaping Multimodal AI</a> </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#070bff"> <a href="https://eccv.ecva.net/" rel="external nofollow noopener" target="_blank">ECCV</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/vitpcm.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vitpcm.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rossetti2022max" class="col-sm-8"> <div class="title">Max Pooling with Vision Transformers Reconciles Class and Shape in Weakly Supervised Semantic Segmentation</div> <div class="author"> <em>Simone Rossetti<sup>‚Ä†*</sup></em>, Damiano Zappia<sup>*</sup>, <a href="https://scholar.google.com/citations?user=Qaq3N3cAAAAJ&amp;hl" rel="external nofollow noopener" target="_blank">Marta Sanzari<sup>*</sup></a>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Marco Schaerf&lt;sup&gt;‚Ä†*&lt;/sup&gt;, Fiora Pirri&lt;sup&gt;‚Ä†*&lt;/sup&gt;' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="‚Ä† DIAG, Sapienza University of Rome&lt;br&gt;* DeepPlants"> </i> </div> <div class="periodical"> <em>In Computer Vision ‚Äì ECCV</em>. <em>More Information</em> can be <a href="https://github.com/deepplants/ViT-PCM/" rel="external nofollow noopener" target="_blank">found here</a>. , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-031-20056-4_26" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-20056-4_26" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2210.17400" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136900442-supp.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> <a href="https://drive.google.com/file/d/1wcam_nS2Tgs_R_wKyYcbHBxW5Cy_DdEI/view?usp=share_link" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://drive.google.com/file/d/1VrA9T2g8HdYxbVKMP0DbL42Fk8gyRwGu/view?usp=share_link" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1007/978-3-031-20056-4_26" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=aynWg48AAAAJ&amp;citation_for_view=aynWg48AAAAJ:d1gkVwhDpl0C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-99-4285F4?logo=googlescholar&amp;labelColor=beige" alt="99 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Weakly Supervised Semantic Segmentation (WSSS) research has explored many directions to improve the typical pipeline CNN plus class activation maps (CAM) plus refinements, given the image-class label as the only supervision. Though the gap with the fully supervised methods is reduced, further abating the spread seems unlikely within this framework. On the other hand, WSSS methods based on Vision Transformers (ViT) have not yet explored valid alternatives to CAM. ViT features have been shown to retain a scene layout, and object boundaries in self-supervised learning. To confirm these findings, we prove that the advantages of transformers in self-supervised methods are further strengthened by Global Max Pooling (GMP), which can leverage patch features to negotiate pixel-label probability with class probability. This work proposes a new WSSS method dubbed ViT-PCM (ViT Patch-Class Mapping), not based on CAM. The end-to-end presented network learns with a single optimization process, refined shape and proper localization for segmentation masks. Our model outperforms the state-of-the-art on baseline pseudo-masks (BPM), where we achieve 69.3% mIoU on PascalVOC 2012 val set. We show that our approach has the least set of parameters, though obtaining higher accuracy than all other approaches. In a sentence, quantitative and qualitative results of our method reveal that ViT-PCM is an excellent alternative to CNN-CAM based architectures.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rossetti2022max</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rossetti, Simone and Zappia, Damiano and Sanzari, Marta and Schaerf, Marco and Pirri, Fiora}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Max Pooling with Vision Transformers Reconciles Class and Shape in Weakly Supervised Semantic Segmentation}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Computer Vision -- ECCV}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springer Nature Switzerland}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Cham}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{446--463}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-3-031-20056-4}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/978-3-031-20056-4_26}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#FFA500"> <a href="https://iccv.thecvf.com/" rel="external nofollow noopener" target="_blank">ICCV</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/agm.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="agm.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="sama2023a" class="col-sm-8"> <div class="title">A new Large Dataset and a Transfer Learning Methodology for Plant Phenotyping in Vertical Farms</div> <div class="author"> <a href="https://scholar.google.com/citations?hl=en&amp;user=dma4OokAAAAJ" rel="external nofollow noopener" target="_blank">Nico Sam√†<sup>*</sup></a>, Etienne David¬∞, <em>Simone Rossetti<sup>‚Ä†*</sup></em>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Alessandro Antona¬∞, Benjamin Franchetti¬∞, Fiora Pirri&lt;sup&gt;‚Ä†*&lt;/sup&gt;' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="‚Ä† DIAG, Sapienza University of Rome&lt;br&gt;* DeepPlantse&lt;br&gt;¬∞ AgricolaModerna"> </i> </div> <div class="periodical"> <em>In IEEE/CVF International Conference on Computer Vision Workshops</em>. <em>More Information</em> can be <a href="https://github.com/deepplants/agm-plant-classification" rel="external nofollow noopener" target="_blank">found here</a>. , Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ICCVW60793.2023.00061" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Sama_A_new_Large_Dataset_and_a_Transfer_Learning_Methodology_for_ICCVW_2023_paper.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/papers/Sama_A_new_Large_Dataset_and_a_Transfer_Learning_Methodology_for_ICCVW_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/ICCVW60793.2023.00061" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=aynWg48AAAAJ&amp;citation_for_view=aynWg48AAAAJ:2osOgNQ5qMEC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-12-4285F4?logo=googlescholar&amp;labelColor=beige" alt="12 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Vertical farming has emerged as a solution to enhance crop cultivation efficiency and overcome limitations in conventional farming methods. Yet, abiotic stresses significantly impact crop quality and increase the risk of food loss. The integration of advanced automation, sensor technology, and deep learning models offers a promising solution for quality monitoring addressing the limitations of stress-specific approaches. Due to the large range of possible quality issues, there is a need for a general method. This study proposes a new plant canopy dataset, dubbed AGM of 1M images, annotated with 18 classes, an in-depth analysis of its quality for its use in transfer learning, and a methodology for detecting canopy stresses in vertical farming. The present study trains ViTbase8, ViTsmall8, and ResNet50 both on ImageNet and the proposed dataset on crop classification. Features from AGM and ImageNet are used for a downstream task on healthy and stress detection using a small annotated validation dataset obtaining 0.97%, 0.93%, and 0.92% best accuracy with the AGM features. We compare with standard datasets like Cassava, PlantDoc, and RicePlant obtaining significant accuracy. This research contributes to improved crop quality, prolonged shelf life, and optimized nutrient content in vertical farming, enhancing our understanding of abiotic stress management.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">sama2023a</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Sam√†, Nico and David¬∞, Etienne and Rossetti, Simone and Antona¬∞, Alessandro and Franchetti¬∞, Benjamin and Pirri, Fiora}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A new Large Dataset and a Transfer Learning Methodology for Plant Phenotyping in Vertical Farms}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/CVF International Conference on Computer Vision Workshops}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{540-551}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ICCVW60793.2023.00061}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#8F00FF"> <a href="https://neurips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/hierarchy.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="hierarchy.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rossetti2024hierarchy" class="col-sm-8"> <div class="title">Hierarchy-Agnostic Unsupervised Segmentation: Parsing Semantic Image Structure</div> <div class="author"> <em>Simone Rossetti<sup>‚Ä†*</sup></em> and <a href="https://scholar.google.com/citations?user=VkYspW0AAAAJ&amp;hl" rel="external nofollow noopener" target="_blank">Fiora Pirri<sup>‚Ä†*</sup></a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="‚Ä† DIAG, Sapienza University of Rome&lt;br&gt;* DeepPlants"> </i> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>. <em>More Information</em> can be <a href="https://github.com/deepplants/recursive-deep-spectral-clustering" rel="external nofollow noopener" target="_blank">found here</a>. , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.52202/079017-3139" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://proceedings.neurips.cc/paper/2024/hash/b31c332c4cebcec31b788400b47c94b3-Abstract.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://proceedings.neurips.cc/paper/2024/file/b31c332c4cebcec31b788400b47c94b3-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/b31c332c4cebcec31b788400b47c94b3-Supplemental-Conference.zip" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Supp</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.52202/079017-3139" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=aynWg48AAAAJ&amp;citation_for_view=aynWg48AAAAJ:qjMakFHDy7sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-1-4285F4?logo=googlescholar&amp;labelColor=beige" alt="1 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Unsupervised semantic segmentation aims to discover groupings within images, capturing objects‚Äô view-invariance without external supervision. Moreover, this task is inherently ambiguous due to the varying levels of semantic granularity. Existing methods often bypass this ambiguity using dataset-specific priors. In our research, we address this ambiguity head-on and provide a universal tool for pixel-level semantic parsing of images guided by the latent representations encoded in self-supervised models. We introduce a novel algebraic approach that recursively decomposes an image into nested subgraphs, dynamically estimating their count and ensuring clear separation.The innovative approach identifies scene-specific primitives and constructs a hierarchy-agnostic tree of semantic regions from the image pixels. The model captures fine and coarse semantic details, producing a nuanced and unbiased segmentation. We present a new metric for estimating the quality of the semantic segmentation of discovered elements on different levels of the hierarchy. The metric validates the intrinsic nature of the compositional relations among parts, objects, and scenes in a hierarchy-agnostic domain. Our results prove the power of this methodology, uncovering semantic regions without prior definitions and scaling effectively across various datasets. This robust framework for unsupervised image segmentation proves more accurate semantic hierarchical relationships between scene elements than traditional algorithms. The experiments underscore its potential for broad applicability in image analysis tasks, showcasing its ability to deliver a detailed and unbiased segmentation that surpasses existing unsupervised methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">rossetti2024hierarchy</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rossetti, Simone and Pirri, Fiora}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hierarchy-Agnostic Unsupervised Segmentation: Parsing Semantic Image Structure}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.52202/079017-3139}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="/assets/pdf/mycv.pdf" title="CV" target="_blank"><i class="ai ai-cv"></i></a> <a href="mailto:%73%69%6D%6F%6E%65.%72%6F%73%73%65%74%74%69@%6C%69%76%65.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> <a href="https://github.com/rossettisimone" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/rossettisimone" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://orcid.org/0000-0002-5344-7872" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=aynWg48AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> </div> <div class="contact-note">Best way to reach me: email. </div> </div> <div class="newsletter-form-container" style="margin: 20px"> <form class="newsletter-form" action="https://app.loops.so/api/newsletter-form/" method="POST" style="justify-content: center"> <input class="newsletter-form-input" name="newsletter-form-input" type="email" placeholder="user@example.com" required=""> <button type="submit" class="newsletter-form-button" style="justify-content: center"> subscribe </button> <button type="button" class="newsletter-loading-button" style="justify-content: center"> Please wait... </button> </form> <div class="newsletter-success" style="justify-content: center"> <p class="newsletter-success-message">You're subscribed!</p> </div> <div class="newsletter-error" style="justify-content: center"> <p class="newsletter-error-message">Oops! Something went wrong, please try again</p> </div> <button class="newsletter-back-button" type="button" onmouseout='this.style.textDecoration="none"' onmouseover='this.style.textDecoration="underline"'> ‚Üê Back </button> </div> <noscript> <style>.newsletter-form-container{display:none}</style> </noscript> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2026 Simone Rossetti. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=6f508d74becd347268a7f822bca7309d"></script> <script defer src="/assets/js/newsletter.js?v=c3d0931971ee96e9df74ba70526c3130"></script> </body> </html>