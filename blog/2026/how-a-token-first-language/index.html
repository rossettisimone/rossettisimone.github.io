<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How a Token-First Language Axis Is Reshaping Multimodal AI | Simone Rossetti </title> <meta name="author" content="Simone Rossetti"> <meta name="description" content="Discrete tokens, any-to-any learning, and why language is becoming the operating system of multimodal AI."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rossettisimone.github.io/blog/2026/how-a-token-first-language/"> <script src="/assets/js/theme.js?v=a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Simone</span> Rossetti </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">How a Token-First Language Axis Is Reshaping Multimodal AI</h1> <p class="post-meta"> Created on January 21, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   <a href="/blog/tag/multimodal"> <i class="fa-solid fa-hashtag fa-sm"></i> multimodal</a>   <a href="/blog/tag/foundation-models"> <i class="fa-solid fa-hashtag fa-sm"></i> foundation-models</a>   <a href="/blog/tag/vision-language"> <i class="fa-solid fa-hashtag fa-sm"></i> vision-language</a>   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a>   <a href="/blog/category/ai"> <i class="fa-solid fa-tag fa-sm"></i> ai</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#why-we-need-foundational-multimodal-models">Why We Need Foundational Multimodal Models</a></li> <li class="toc-entry toc-h2"><a href="#unifying-modalities-through-discrete-tokens">Unifying Modalities Through Discrete Tokens</a></li> <li class="toc-entry toc-h2"><a href="#any-to-any-generation-tasks-become-queries">Any-to-Any Generation: Tasks Become Queries</a></li> <li class="toc-entry toc-h2"><a href="#fine-grained-control--steerability">Fine-Grained Control &amp; Steerability</a></li> <li class="toc-entry toc-h2"><a href="#language-co-training--improved-understanding">Language Co-Training &amp; Improved Understanding</a></li> <li class="toc-entry toc-h2"><a href="#beyond-generation-retrieval-and-evaluation">Beyond Generation: Retrieval and Evaluation</a></li> <li class="toc-entry toc-h2"><a href="#authors-and-official-attribution">Authors and Official Attribution</a></li> <li class="toc-entry toc-h2"> <a href="#conclusion-foundations-built-but-the-frontier-remains">Conclusion: Foundations Built, But the Frontier Remains</a> <ul> <li class="toc-entry toc-h3"><a href="#what-4m21-does-not-doyet">What 4M‑21 Does Not Do—Yet</a></li> <li class="toc-entry toc-h3"><a href="#the-road-ahead-directions-to-watch">The Road Ahead: Directions to Watch</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#references">References</a></li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="why-we-need-foundational-multimodal-models">Why We Need Foundational Multimodal Models</h2> <p>Today’s AI landscape is fractured. Vision models, language models, and geometric/semantic prediction pipelines each solve narrow tasks. Yet real-world systems—from robotics to complex retrieval agents—require <strong>joint reasoning across perception, structure, and semantics</strong>.</p> <p>Current multimodal systems rely on:</p> <ul> <li>task-specific heads,</li> <li>bespoke pipelines,</li> <li>engineering-heavy loss balancing.</li> </ul> <p>This approach <em>does not scale</em>. It fragments learning into silos instead of enabling true <strong>compositional understanding and control</strong>.</p> <p>We need what language models gave us for text: a <strong>foundation model</strong> whose representations can be queried, composed, and extended without redesigning the architecture for every new task or modality.</p> <p>This is exactly the paradigm shift that <a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank"><em>4M: Massively Multimodal Masked Modeling</em></a> and its scaled variant <a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank"><em>4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities</em></a> introduce.</p> <blockquote> <p>“A framework for training any-to-any multimodal foundation models. Scalable. Open-sourced. Across tens of modalities and tasks.” — <a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank"><em>4M official page</em></a></p> </blockquote> <h2 id="unifying-modalities-through-discrete-tokens">Unifying Modalities Through Discrete Tokens</h2> <p><img src="https://storage.googleapis.com/four_m_site/images/4M_modalities.svg" alt="Modalities overview"><br> <em>4M trains a single model to predict any modality from any subset of others using discrete tokenization.</em> (<a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank">Source: 4M project page</a>)</p> <p>At the core of 4M and 4M-21 is <strong>discrete tokenization</strong>:</p> <ul> <li>Images, depth maps, geometry, semantic maps, captions, and feature maps all become sequences of discrete tokens.</li> <li>A <strong>single transformer encoder-decoder</strong> predicts masked tokens from visible ones.</li> <li>There are no task-specific heads or bespoke objectives.</li> </ul> <p>The official site summarizes this succinctly:</p> <blockquote> <p>“By tokenizing modalities into sequences of discrete tokens, we can train a single unified Transformer encoder-decoder on a diverse set of modalities.” (<a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank">Source: 4M project page</a>)</p> </blockquote> <p>This token-centric representation is the <strong>unifying abstraction</strong> that lets one model handle diverse data types without architectural surgery.</p> <h2 id="any-to-any-generation-tasks-become-queries">Any-to-Any Generation: Tasks Become Queries</h2> <p><img src="https://storage.googleapis.com/four_m_site/videos/4M_chained_generation.mp4#t=32.5" alt="4M chained generation animation"><br> <em>4M can generate any modality from any conditioning set in a self-consistent chained manner.</em> (<a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank">Source: 4M project page</a>)</p> <p>One of the most striking capabilities shown on the official page is <strong>any-to-any generation</strong>. Instead of solving fixed tasks like “caption this image” or “predict depth from color”, 4M generates all modalities from whichever subset you choose.</p> <p>The generation works by:</p> <ol> <li>Predicting missing tokens for one modality.</li> <li>Feeding fully generated modalities back into the model.</li> <li>Repeating until all target modalities are generated.</li> </ol> <p>This yields <em>self-consistent, multimodal predictions</em> without loss balancing or head selection (see <a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank">4M project page</a>).</p> <h2 id="fine-grained-control--steerability">Fine-Grained Control &amp; Steerability</h2> <p><img src="https://storage.googleapis.com/four_m_site/images/4M_editing.svg" alt="Multimodal editing examples"><br> <em>4M supports multimodal editing and fine-grained control, such as bounding box–guided RGB generation.</em> (<a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank">Source: 4M project page</a>)</p> <p>Because 4M represents all data in token form, it supports:</p> <ul> <li>partial conditioning (e.g., captions + bounding boxes),</li> <li>semantic and geometric guidance,</li> <li>compositional weighting of conditions.</li> </ul> <p>The official visuals demonstrate how changing a <strong>bounding box</strong> input can reorganize the RGB output—semantic edits become natural rather than hacky (<a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank">4M project page</a>).</p> <h2 id="language-co-training--improved-understanding">Language Co-Training &amp; Improved Understanding</h2> <p><img src="https://storage.googleapis.com/four_m_site/images/4M_text_understanding.svg" alt="Improved text understanding"><br> <em>4M-21 models co-trained with text corpora show stronger text understanding than smaller multimodal variants.</em> (<a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank">Source: 4M project page</a>)</p> <p>4M-21 extends 4M by co-training on large text corpora and incorporating <strong>language as a structural modality</strong> rather than a side condition. The official site notes:</p> <blockquote> <p>“4M models trained on a larger variety of modalities and co-trained on a text corpus exhibit a higher degree of text understanding.” (<a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank">Source: 4M project page</a>)</p> </blockquote> <p>This positions language not just as a human interface, but as a <strong>semantic scaffold</strong> the model uses internally to organize multimodal representations.</p> <h2 id="beyond-generation-retrieval-and-evaluation">Beyond Generation: Retrieval and Evaluation</h2> <p>The official project page also highlights:</p> <ul> <li> <strong>Multimodal retrieval</strong> by predicting global embeddings from any modality (<a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank">4M project page</a>).</li> <li> <strong>Out-of-the-box evaluations</strong> showing 4M-21’s performance often matches or surpasses specialist baselines and multimodal competitors like Unified-IO (<a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank">4M project page</a>).</li> </ul> <h2 id="authors-and-official-attribution">Authors and Official Attribution</h2> <p>This work is the result of collaboration between EPFL and Apple researchers:</p> <blockquote> <p><em>David Mizrahi, Roman Bachmann, Oğuzhan Fatih Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, Amir Zamir… and colleagues</em> — <em>4M &amp; 4M-21 teams</em>.</p> </blockquote> <p>Both papers are available from the <a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank">official 4M project site</a>:</p> <ul> <li> <strong>4M: Massively Multimodal Masked Modeling</strong> (NeurIPS 2023) — Mizrahi et al., 2023</li> <li> <strong>4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities</strong> (NeurIPS 2024) — Bachmann et al., 2024</li> </ul> <h2 id="conclusion-foundations-built-but-the-frontier-remains">Conclusion: Foundations Built, But the Frontier Remains</h2> <p>4M and 4M‑21 mark a turning point in multimodal AI. They show that:</p> <ul> <li><strong>Unified token spaces work across dozens of modalities</strong></li> <li><strong>Language can serve as a structural interface, not just a conditioning signal</strong></li> <li><strong>Tasks can emerge from conditioning rather than engineered heads</strong></li> <li><strong>Models can scale without performance collapse, even as modalities triple</strong></li> </ul> <p>Yet as impressive as these results are, the frontier of <strong>true multimodal intelligence</strong> is still wide open.</p> <h3 id="what-4m21-does-not-doyet">What 4M‑21 Does Not Do—Yet</h3> <p>The project is not a reasoning-first system. It cannot plan, chain steps explicitly, or act autonomously:</p> <ul> <li> <strong>Emergent reasoning is limited</strong>: There’s no explicit chain-of-thought or planning; constraint satisfaction occurs implicitly across tokens.</li> <li> <strong>Tokenization bottlenecks exist</strong>: Discretization is lossy, which limits fidelity for complex modalities.</li> <li> <strong>Dataset alignment is partial</strong>: Some modalities and datasets are only loosely coordinated, leaving room for inconsistencies in training.</li> </ul> <p>In other words, 4M‑21 is a <strong>foundation backbone</strong>, not an agent or cognitive system. It lays the groundwork, but the “thinking” part—planning, instruction-following, and compositional reasoning—is still to come.</p> <h3 id="the-road-ahead-directions-to-watch">The Road Ahead: Directions to Watch</h3> <p>The official project and research notes point to several promising avenues:</p> <ol> <li> <p><strong>Better tokenization schemes</strong><br> Adaptive, higher-fidelity tokenizers could reduce reconstruction loss and improve fine-grained multimodal generation.</p> </li> <li> <p><strong>Explicit reasoning objectives</strong><br> Integrating constraint-based or reasoning-centered training could turn implicit consistency into explicit reasoning capabilities.</p> </li> <li> <p><strong>Instruction tuning over token sequences</strong><br> Just like LLMs benefit from instruction fine-tuning, multimodal backbones could learn to follow structured multimodal instructions across domains.</p> </li> <li> <p><strong>Integration with agentic architectures</strong><br> Combining unified token spaces with LLM-style planners, memory modules, or embodied agents could finally unlock reasoning and agency in multimodal systems.</p> </li> </ol> <p>In short, 4M‑21 has built the <strong>scalable, unified foundation</strong>, and the next frontier is layering <strong>reasoning, instruction-following, and agency</strong> on top.</p> <blockquote> <p>The lesson for the field is clear: multimodal AI has crossed the threshold of scalability and unification—but intelligence still has a way to go. The foundation is there; now comes the building.</p> </blockquote> <hr> <h2 id="references">References</h2> <ul> <li> <a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank">4M official project page</a>, <em>Massively Multimodal Masked Modeling</em>, EPFL / Apple Research.</li> <li>Bachmann, R., et al. (2024). <em>4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities</em>. Advances in Neural Information Processing Systems (NeurIPS 2024). Available at <a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank">4m.epfl.ch</a>.</li> <li>Mizrahi, D., et al. (2023). <em>4M: Massively Multimodal Masked Modeling</em>. Advances in Neural Information Processing Systems (NeurIPS 2023). Available at <a href="https://4m.epfl.ch/" rel="external nofollow noopener" target="_blank">4m.epfl.ch</a>.</li> </ul> </div> </article> <br> <hr> <br> If you found this useful, please cite this as: <blockquote> <p>Rossetti, Simone (Jan 2026). How a Token-First Language Axis Is Reshaping Multimodal AI. https://rossettisimone.github.io.</p> </blockquote> <p>or as a BibTeX entry:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">rossetti2026how-a-token-first-language-axis-is-reshaping-multimodal-ai</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{How a Token-First Language Axis Is Reshaping Multimodal AI}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Rossetti, Simone}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2026}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Jan}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://rossettisimone.github.io/blog/2026/how-a-token-first-language/}</span>
<span class="p">}</span>
</code></pre></div></div> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/the-most-insane-machine/">The Most Insane Machine on Earth: Inside ASML’s EUV Lithography Systems</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/the-end-of-crucial/">The End of Crucial: How the Memory Industry Is Abandoning the Consumer Market</a> </li> <p class="mb-2" style="margin-top: 1.5rem !important">Subscribe to be notified of future articles:</p> <div class="newsletter-form-container"> <form class="newsletter-form" action="https://app.loops.so/api/newsletter-form/" method="POST" style="justify-content: flex-start"> <input class="newsletter-form-input" name="newsletter-form-input" type="email" placeholder="user@example.com" required=""> <button type="submit" class="newsletter-form-button" style="justify-content: flex-start"> subscribe </button> <button type="button" class="newsletter-loading-button" style="justify-content: flex-start"> Please wait... </button> </form> <div class="newsletter-success" style="justify-content: flex-start"> <p class="newsletter-success-message">You're subscribed!</p> </div> <div class="newsletter-error" style="justify-content: flex-start"> <p class="newsletter-error-message">Oops! Something went wrong, please try again</p> </div> <button class="newsletter-back-button" type="button" onmouseout='this.style.textDecoration="none"' onmouseover='this.style.textDecoration="underline"'> ← Back </button> </div> <noscript> <style>.newsletter-form-container{display:none}</style> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Simone Rossetti. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=6f508d74becd347268a7f822bca7309d"></script> <script defer src="/assets/js/newsletter.js?v=c3d0931971ee96e9df74ba70526c3130"></script> </body> </html>