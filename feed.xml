<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://rossettisimone.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rossettisimone.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-21T18:17:21+00:00</updated><id>https://rossettisimone.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Simone Rossetti — AI Research Engineer working on computer vision and vision-language learning. </subtitle><entry><title type="html">How a Token-First Language Axis Is Reshaping Multimodal AI</title><link href="https://rossettisimone.github.io/blog/2026/how-a-token-first-language/" rel="alternate" type="text/html" title="How a Token-First Language Axis Is Reshaping Multimodal AI"/><published>2026-01-21T11:00:00+00:00</published><updated>2026-01-21T11:00:00+00:00</updated><id>https://rossettisimone.github.io/blog/2026/how-a-token-first-language</id><content type="html" xml:base="https://rossettisimone.github.io/blog/2026/how-a-token-first-language/"><![CDATA[<h2 id="why-we-need-foundational-multimodal-models">Why We Need Foundational Multimodal Models</h2> <p>Today’s AI landscape is fractured. Vision models, language models, and geometric/semantic prediction pipelines each solve narrow tasks. Yet real-world systems—from robotics to complex retrieval agents—require <strong>joint reasoning across perception, structure, and semantics</strong>.</p> <p>Current multimodal systems rely on:</p> <ul> <li>task-specific heads,</li> <li>bespoke pipelines,</li> <li>engineering-heavy loss balancing.</li> </ul> <p>This approach <em>does not scale</em>. It fragments learning into silos instead of enabling true <strong>compositional understanding and control</strong>.</p> <p>We need what language models gave us for text: a <strong>foundation model</strong> whose representations can be queried, composed, and extended without redesigning the architecture for every new task or modality.</p> <p>This is exactly the paradigm shift that <a href="https://4m.epfl.ch/"><em>4M: Massively Multimodal Masked Modeling</em></a> and its scaled variant <a href="https://4m.epfl.ch/"><em>4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities</em></a> introduce.</p> <blockquote> <p>“A framework for training any-to-any multimodal foundation models. Scalable. Open-sourced. Across tens of modalities and tasks.” — <a href="https://4m.epfl.ch/"><em>4M official page</em></a></p> </blockquote> <h2 id="unifying-modalities-through-discrete-tokens">Unifying Modalities Through Discrete Tokens</h2> <p><img src="https://storage.googleapis.com/four_m_site/images/4M_modalities.svg" alt="Modalities overview"/><br/> <em>4M trains a single model to predict any modality from any subset of others using discrete tokenization.</em> (<a href="https://4m.epfl.ch/">Source: 4M project page</a>)</p> <p>At the core of 4M and 4M-21 is <strong>discrete tokenization</strong>:</p> <ul> <li>Images, depth maps, geometry, semantic maps, captions, and feature maps all become sequences of discrete tokens.</li> <li>A <strong>single transformer encoder-decoder</strong> predicts masked tokens from visible ones.</li> <li>There are no task-specific heads or bespoke objectives.</li> </ul> <p>The official site summarizes this succinctly:</p> <blockquote> <p>“By tokenizing modalities into sequences of discrete tokens, we can train a single unified Transformer encoder-decoder on a diverse set of modalities.” (<a href="https://4m.epfl.ch/">Source: 4M project page</a>)</p> </blockquote> <p>This token-centric representation is the <strong>unifying abstraction</strong> that lets one model handle diverse data types without architectural surgery.</p> <h2 id="any-to-any-generation-tasks-become-queries">Any-to-Any Generation: Tasks Become Queries</h2> <p><img src="https://storage.googleapis.com/four_m_site/videos/4M_chained_generation.mp4#t=32.5" alt="4M chained generation animation"/><br/> <em>4M can generate any modality from any conditioning set in a self-consistent chained manner.</em> (<a href="https://4m.epfl.ch/">Source: 4M project page</a>)</p> <p>One of the most striking capabilities shown on the official page is <strong>any-to-any generation</strong>. Instead of solving fixed tasks like “caption this image” or “predict depth from color”, 4M generates all modalities from whichever subset you choose.</p> <p>The generation works by:</p> <ol> <li>Predicting missing tokens for one modality.</li> <li>Feeding fully generated modalities back into the model.</li> <li>Repeating until all target modalities are generated.</li> </ol> <p>This yields <em>self-consistent, multimodal predictions</em> without loss balancing or head selection (see <a href="https://4m.epfl.ch/">4M project page</a>).</p> <h2 id="fine-grained-control--steerability">Fine-Grained Control &amp; Steerability</h2> <p><img src="https://storage.googleapis.com/four_m_site/images/4M_editing.svg" alt="Multimodal editing examples"/><br/> <em>4M supports multimodal editing and fine-grained control, such as bounding box–guided RGB generation.</em> (<a href="https://4m.epfl.ch/">Source: 4M project page</a>)</p> <p>Because 4M represents all data in token form, it supports:</p> <ul> <li>partial conditioning (e.g., captions + bounding boxes),</li> <li>semantic and geometric guidance,</li> <li>compositional weighting of conditions.</li> </ul> <p>The official visuals demonstrate how changing a <strong>bounding box</strong> input can reorganize the RGB output—semantic edits become natural rather than hacky (<a href="https://4m.epfl.ch/">4M project page</a>).</p> <h2 id="language-co-training--improved-understanding">Language Co-Training &amp; Improved Understanding</h2> <p><img src="https://storage.googleapis.com/four_m_site/images/4M_text_understanding.svg" alt="Improved text understanding"/><br/> <em>4M-21 models co-trained with text corpora show stronger text understanding than smaller multimodal variants.</em> (<a href="https://4m.epfl.ch/">Source: 4M project page</a>)</p> <p>4M-21 extends 4M by co-training on large text corpora and incorporating <strong>language as a structural modality</strong> rather than a side condition. The official site notes:</p> <blockquote> <p>“4M models trained on a larger variety of modalities and co-trained on a text corpus exhibit a higher degree of text understanding.” (<a href="https://4m.epfl.ch/">Source: 4M project page</a>)</p> </blockquote> <p>This positions language not just as a human interface, but as a <strong>semantic scaffold</strong> the model uses internally to organize multimodal representations.</p> <h2 id="beyond-generation-retrieval-and-evaluation">Beyond Generation: Retrieval and Evaluation</h2> <p>The official project page also highlights:</p> <ul> <li><strong>Multimodal retrieval</strong> by predicting global embeddings from any modality (<a href="https://4m.epfl.ch/">4M project page</a>).</li> <li><strong>Out-of-the-box evaluations</strong> showing 4M-21’s performance often matches or surpasses specialist baselines and multimodal competitors like Unified-IO (<a href="https://4m.epfl.ch/">4M project page</a>).</li> </ul> <h2 id="authors-and-official-attribution">Authors and Official Attribution</h2> <p>This work is the result of collaboration between EPFL and Apple researchers:</p> <blockquote> <p><em>David Mizrahi, Roman Bachmann, Oğuzhan Fatih Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, Amir Zamir… and colleagues</em> — <em>4M &amp; 4M-21 teams</em>.</p> </blockquote> <p>Both papers are available from the <a href="https://4m.epfl.ch/">official 4M project site</a>:</p> <ul> <li><strong>4M: Massively Multimodal Masked Modeling</strong> (NeurIPS 2023) — Mizrahi et al., 2023</li> <li><strong>4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities</strong> (NeurIPS 2024) — Bachmann et al., 2024</li> </ul> <h2 id="conclusion-foundations-built-but-the-frontier-remains">Conclusion: Foundations Built, But the Frontier Remains</h2> <p>4M and 4M‑21 mark a turning point in multimodal AI. They show that:</p> <ul> <li><strong>Unified token spaces work across dozens of modalities</strong></li> <li><strong>Language can serve as a structural interface, not just a conditioning signal</strong></li> <li><strong>Tasks can emerge from conditioning rather than engineered heads</strong></li> <li><strong>Models can scale without performance collapse, even as modalities triple</strong></li> </ul> <p>Yet as impressive as these results are, the frontier of <strong>true multimodal intelligence</strong> is still wide open.</p> <h3 id="what-4m21-does-not-doyet">What 4M‑21 Does Not Do—Yet</h3> <p>The project is not a reasoning-first system. It cannot plan, chain steps explicitly, or act autonomously:</p> <ul> <li><strong>Emergent reasoning is limited</strong>: There’s no explicit chain-of-thought or planning; constraint satisfaction occurs implicitly across tokens.</li> <li><strong>Tokenization bottlenecks exist</strong>: Discretization is lossy, which limits fidelity for complex modalities.</li> <li><strong>Dataset alignment is partial</strong>: Some modalities and datasets are only loosely coordinated, leaving room for inconsistencies in training.</li> </ul> <p>In other words, 4M‑21 is a <strong>foundation backbone</strong>, not an agent or cognitive system. It lays the groundwork, but the “thinking” part—planning, instruction-following, and compositional reasoning—is still to come.</p> <h3 id="the-road-ahead-directions-to-watch">The Road Ahead: Directions to Watch</h3> <p>The official project and research notes point to several promising avenues:</p> <ol> <li> <p><strong>Better tokenization schemes</strong><br/> Adaptive, higher-fidelity tokenizers could reduce reconstruction loss and improve fine-grained multimodal generation.</p> </li> <li> <p><strong>Explicit reasoning objectives</strong><br/> Integrating constraint-based or reasoning-centered training could turn implicit consistency into explicit reasoning capabilities.</p> </li> <li> <p><strong>Instruction tuning over token sequences</strong><br/> Just like LLMs benefit from instruction fine-tuning, multimodal backbones could learn to follow structured multimodal instructions across domains.</p> </li> <li> <p><strong>Integration with agentic architectures</strong><br/> Combining unified token spaces with LLM-style planners, memory modules, or embodied agents could finally unlock reasoning and agency in multimodal systems.</p> </li> </ol> <p>In short, 4M‑21 has built the <strong>scalable, unified foundation</strong>, and the next frontier is layering <strong>reasoning, instruction-following, and agency</strong> on top.</p> <blockquote> <p>The lesson for the field is clear: multimodal AI has crossed the threshold of scalability and unification—but intelligence still has a way to go. The foundation is there; now comes the building.</p> </blockquote> <hr/> <h2 id="references">References</h2> <ul> <li><a href="https://4m.epfl.ch/">4M official project page</a>, <em>Massively Multimodal Masked Modeling</em>, EPFL / Apple Research.</li> <li>Bachmann, R., et al. (2024). <em>4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities</em>. Advances in Neural Information Processing Systems (NeurIPS 2024). Available at <a href="https://4m.epfl.ch/">4m.epfl.ch</a>.</li> <li>Mizrahi, D., et al. (2023). <em>4M: Massively Multimodal Masked Modeling</em>. Advances in Neural Information Processing Systems (NeurIPS 2023). Available at <a href="https://4m.epfl.ch/">4m.epfl.ch</a>.</li> </ul>]]></content><author><name></name></author><category term="research"/><category term="ai"/><category term="ai"/><category term="multimodal"/><category term="foundation-models"/><category term="vision-language"/><category term="deep-learning"/><summary type="html"><![CDATA[Discrete tokens, any-to-any learning, and why language is becoming the operating system of multimodal AI.]]></summary></entry><entry><title type="html">The End of Crucial: How the Memory Industry Is Abandoning the Consumer Market</title><link href="https://rossettisimone.github.io/blog/2026/the-end-of-crucial/" rel="alternate" type="text/html" title="The End of Crucial: How the Memory Industry Is Abandoning the Consumer Market"/><published>2026-01-20T11:00:00+00:00</published><updated>2026-01-20T11:00:00+00:00</updated><id>https://rossettisimone.github.io/blog/2026/the-end-of-crucial</id><content type="html" xml:base="https://rossettisimone.github.io/blog/2026/the-end-of-crucial/"><![CDATA[<p>For nearly thirty years, <strong>Crucial</strong> represented a rare constant in consumer technology: reliable RAM and SSDs, fair pricing, and minimal theatrics.</p> <p>That era is ending.</p> <p>On <strong>3 December 2025</strong>, <strong>Micron Technology Inc. (Nasdaq: MU) announced its decision to exit the consumer memory and storage market</strong>, discontinuing <strong>Crucial-branded products by February 2026</strong>.</p> <p>Shares of the company fell by <strong>roughly 2–3% in afternoon trading</strong> following the announcement, reflecting short-term market reaction rather than a reassessment of Micron’s long-term fundamentals.</p> <p>This is not the story of a declining brand.<br/> It is the story of an industry that has <strong>decided mass-market hardware is no longer strategic</strong>.</p> <hr/> <h2 id="crucials-role-in-the-pc-ecosystem">Crucial’s role in the PC ecosystem</h2> <p>Founded in 1996 as Micron’s consumer arm, Crucial became a pillar of:</p> <ul> <li>DIY PC building</li> <li>affordable workstation upgrades</li> <li>predictable memory compatibility across platforms</li> </ul> <p>Crucial’s competitive advantage was not peak performance, but <strong>trust</strong>.<br/> For many users, it was the default choice precisely because it removed uncertainty from hardware upgrades.</p> <p>Its disappearance leaves a <strong>structural gap</strong>, not merely a branding void.</p> <p><em>Clarification:</em> while consumer brands assemble and market memory modules, <strong>pricing power and supply control ultimately reside with DRAM and NAND manufacturers</strong>. Crucial’s role was downstream; the strategic decision was upstream.</p> <hr/> <h2 id="microns-decision-explicit-and-strategic">Micron’s decision: explicit and strategic</h2> <p>Micron’s announcement left little room for interpretation.</p> <blockquote> <p>“The AI-driven growth in the data center has led to a surge in demand for memory and storage.<br/> Micron has made the difficult decision to exit the Crucial consumer business in order to improve supply and support for our larger, strategic customers in faster-growing segments.”</p> <p>— <em>Sumit Sadana, EVP and Chief Business Officer, Micron Technology</em></p> </blockquote> <p><strong>Source:</strong><br/> Micron Investor Relations<br/> <a href="https://investors.micron.com/news-releases/news-release-details/micron-announces-exit-crucial-consumer-business/">https://investors.micron.com/news-releases/news-release-details/micron-announces-exit-crucial-consumer-business/</a></p> <p>Crucial shipments will continue through Micron’s <strong>fiscal Q2 2026</strong>, and warranties will be honoured.<br/> However, <strong>no further consumer-focused development is planned</strong>.</p> <p>This represents a <strong>long-term reallocation of capital, production capacity, and R&amp;D</strong>, not a temporary retrenchment.</p> <hr/> <h2 id="this-is-not-an-isolated-case">This is not an isolated case</h2> <p>Micron’s move aligns with a broader industry realignment.</p> <p>According to <strong>Reuters</strong>, major memory manufacturers — including <em>Samsung</em> and <em>SK Hynix</em> — are increasingly prioritising <strong>high-performance memory for AI and data centres</strong>, where margins are structurally higher and contracts are longer-term.</p> <p><strong>Source:</strong><br/> Reuters, <em>Micron to exit Crucial consumer memory business</em><br/> <a href="https://www.reuters.com/business/micron-exit-crucial-consumer-memory-business-2025-12-03/">https://www.reuters.com/business/micron-exit-crucial-consumer-memory-business-2025-12-03/</a> The Crucial exit is therefore best understood as a <strong>leading indicator</strong>, not an anomaly.</p> <hr/> <h2 id="memory-pricing-is-rising-but-not-for-the-reasons-consumers-are-told">Memory pricing is rising, but not for the reasons consumers are told</h2> <p>According to <strong>TrendForce</strong> analysis published in December 2025, the sharp increase in memory prices projected for <strong>Q1 2026</strong> is <strong>not driven by a recovery in consumer demand</strong>.</p> <p>Instead, it reflects a <strong>deliberate and structural reallocation of supply</strong> toward <strong>server and AI-centric applications</strong>.</p> <p><strong>Source:</strong><br/> TrendForce via PR Newswire<br/> <a href="https://www.prnewswire.com/news-releases/memory-makers-prioritize-server-applications-driving-across-the-board-price-increases-in-1q26-says-trendforce-302652560.html">https://www.prnewswire.com/news-releases/memory-makers-prioritize-server-applications-driving-across-the-board-price-increases-in-1q26-says-trendforce-302652560.html</a></p> <p>TrendForce explicitly states that DRAM suppliers are shifting <strong>advanced process capacity</strong> toward <strong>server DRAM and High Bandwidth Memory (HBM)</strong> to support AI server deployments. This prioritisation tightens availability across all other segments — including those where demand is flat or weakening.</p> <p>The following graphic from TrendForce summarises projected <strong>quarter-on-quarter contract price changes from 4Q25 to 1Q26</strong>:</p> <div style="max-width: 100%; height: auto;"> <img src="https://mma.prnewswire.com/media/2854326/image_800964_36645205.jpg?p=publish" alt="4Q25–1Q26 DRAM &amp; NAND Flash Contract Price Projections" style="width: 100%; height: auto; display: block;"/> </div> <p><em>Source: TrendForce / PR Newswire, January 2026</em></p> <h3 id="what-the-data-shows">What the data shows</h3> <p>Several dynamics are immediately visible:</p> <ul> <li><strong>Conventional DRAM prices</strong> accelerate from roughly <strong>45–50% QoQ in 4Q25</strong> to <strong>55–60% QoQ in 1Q26</strong>, despite weak notebook and PC demand.</li> <li><strong>HBM-blended DRAM</strong>, used in AI accelerators, sustains even stronger pricing momentum.</li> <li><strong>NAND Flash prices</strong> remain elevated at <strong>~33–38% QoQ</strong>, reflecting supply optimisation for enterprise SSDs.</li> </ul> <h3 id="what-this-implies">What this implies</h3> <p>Price increases are occurring <strong>even where market fundamentals would normally suggest moderation</strong>.</p> <p>TrendForce notes that while notebook and consumer electronics demand remains constrained, suppliers are <strong>actively limiting shipments</strong> to these channels. Capacity is instead funnelled toward customers able to commit to <strong>large, long-term, high-margin contracts</strong> — primarily hyperscalers and AI infrastructure operators.</p> <p>To make this divergence explicit, the chart below compares <strong>projected 1Q26 contract price increases by segment</strong>, using only TrendForce-reported values.</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": ["Server DRAM", "Conventional DRAM", "Client SSD"],
      "y": [60, 57.5, 40],
      "type": "bar",
      "text": [
        "AI servers and CSP capacity pre-booking",
        "Shipment controls despite weak PC demand",
        "NAND output redirected to enterprise SSDs"
      ],
      "hovertemplate": "%{x}&lt;br&gt;%{y}% QoQ increase&lt;br&gt;%{text}&lt;extra&gt;&lt;/extra&gt;"
    }
  ],
  "layout": {
    "title": {
      "text": "Projected 1Q26 Memory Contract Price Increases by Segment (TrendForce)"
    },
    "yaxis": {
      "title": "Quarter-on-Quarter Price Increase (%)",
      "rangemode": "tozero"
    }
  }
}
</code></pre> <p><em>Methodology note:</em> Conventional DRAM uses the midpoint of the reported 55–60% range. No extrapolation beyond published data has been performed.</p> <p><strong>Conclusion:</strong> the consumer memory market is no longer optimised for <strong>stability or affordability</strong>. It is increasingly treated as <strong>residual output</strong> of an industry tuned for AI workloads.</p> <hr/> <h2 id="what-consumers-will-actually-experience">What consumers will actually experience</h2> <p>The exit of Crucial has immediate, practical consequences:</p> <ul> <li>fewer affordable memory options</li> <li>higher baseline prices for upgrades</li> <li>reduced competitive pressure in the mid-range</li> </ul> <p>According to <strong>Tom’s Hardware</strong>, NAND production costs have risen sharply, pushing SSD and RAM prices upward across consumer segments.</p> <p><strong>Source:</strong> Tom’s Hardware, <em>RAM and SSD prices will continue to rise</em> <a href="https://www.tomshardware.com/pc-components/ram/dont-wait-if-youre-planning-to-upgrade-your-ram-or-ssd-kingston-rep-warns-says-prices-will-continue-to-go-up-nand-costs-up-246-percent">https://www.tomshardware.com/pc-components/ram/dont-wait-if-youre-planning-to-upgrade-your-ram-or-ssd-kingston-rep-warns-says-prices-will-continue-to-go-up-nand-costs-up-246-percent</a></p> <p>This disproportionately affects:</p> <ul> <li>students</li> <li>freelancers</li> <li>small studios</li> <li>mainstream laptop buyers</li> </ul> <hr/> <h2 id="revenue-follows-ai">Revenue follows AI</h2> <p>Micron’s decision reflects where revenue growth is now concentrated.</p> <p>Based on earnings calls and <strong>Reuters financial analysis</strong>, data centre and AI-related memory now account for a rapidly growing share of revenue, while consumer segments stagnate.</p> <h3 id="micron-revenue-focus-consumer-vs-data-centre-qualitative-shift">Micron revenue focus: consumer vs data centre (qualitative shift)</h3> <pre><code class="language-plotly">{
  "data": [
    {
      "type": "bar",
      "name": "Consumer Memory",
      "x": ["Past (Pre-AI boom)", "Current"],
      "y": [1, 0.5]
    },
    {
      "type": "bar",
      "name": "Data Centre &amp; AI",
      "x": ["Past (Pre-AI boom)", "Current"],
      "y": [0.6, 1.2]
    }
  ],
  "layout": {
    "title": {
      "text": "Relative Revenue Emphasis Shift (Illustrative, Source-Based)"
    },
    "yaxis": {
      "title": "Relative Revenue Weight (Indexed)"
    },
    "barmode": "group"
  }
}
</code></pre> <p><em>Important clarification:</em> this chart is <strong>illustrative</strong>, reflecting directional emphasis described in earnings calls rather than exact revenue percentages.</p> <hr/> <h2 id="counterargument-what-if-ai-demand-slows">Counterargument: what if AI demand slows?</h2> <p>A common counterargument is that AI infrastructure demand may normalise, freeing capacity back to consumer markets.</p> <p>However, this assumes manufacturers are willing to retool <strong>advanced process nodes</strong> for <strong>lower-margin consumer products</strong> — an assumption not supported by current capital allocation trends. Once fabs, contracts, and roadmaps are optimised for hyperscale customers, reversion is economically unattractive.</p> <hr/> <h2 id="the-hidden-risk-of-over-concentration">The hidden risk of over-concentration</h2> <p>The semiconductor industry is historically cyclical.</p> <p>By concentrating production and investment almost entirely on AI and enterprise demand:</p> <ul> <li>flexibility decreases</li> <li>exposure to demand normalisation increases</li> <li>consumer trust erodes</li> </ul> <p>Crucial’s exit is therefore not merely a cost decision, but a <strong>strategic bet on the durability of AI-driven demand</strong>.</p> <hr/> <h2 id="conclusion-crucial-is-a-symptom-not-the-disease">Conclusion: Crucial is a symptom, not the disease</h2> <p>Crucial did not disappear because it failed. It disappeared because it <strong>no longer fit an industry optimised for AI margins</strong>.</p> <p>For consumers, this likely means:</p> <ul> <li>higher prices</li> <li>fewer choices</li> <li>longer upgrade cycles</li> </ul> <p>The memory market is being reshaped in real time. The more relevant question may be whether the consumer memory segment will <strong>ever again be treated as a first-class priority</strong> in a post-AI semiconductor industry.</p> <hr/> <h2 id="sources">Sources</h2> <ul> <li>Micron Investor Relations — <a href="https://investors.micron.com/">https://investors.micron.com/</a></li> <li>Reuters, semiconductor and memory market analysis — <a href="https://www.reuters.com/">https://www.reuters.com/</a></li> <li>TrendForce, memory pricing reports — <a href="https://www.trendforce.com/">https://www.trendforce.com/</a></li> <li>Tom’s Hardware, component pricing commentary — <a href="https://www.tomshardware.com/">https://www.tomshardware.com/</a></li> </ul>]]></content><author><name></name></author><category term="technology"/><category term="industry"/><category term="memory"/><category term="ai"/><category term="semiconductors"/><category term="hardware"/><category term="market"/><summary type="html"><![CDATA[Micron’s decision to discontinue Crucial marks a structural shift in the memory industry, driven by AI demand and rising prices.]]></summary></entry></feed>